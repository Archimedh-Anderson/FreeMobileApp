# FreeMobilaChat - Plateforme d'Analyse de Sentiment Multi-Mod√®les

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.51.0-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)

**Application de classification automatique de r√©clamations avec Intelligence Artificielle Multi-Mod√®les**

*M√©moire de Master en Data Science | Classification Multi-KPI | Architecture Production-Ready*

[Installation](#-installation) ‚Ä¢ [Architecture](#-architecture-technique) ‚Ä¢ [Utilisation](#-utilisation) ‚Ä¢ [Documentation](#-documentation-technique)

</div>

---

## [TOC] Table des Mati√®res

1. [Vue d'ensemble du Projet](#vue-densemble-du-projet)
2. [Contexte et Probl√©matique](#contexte-et-probl√©matique)
3. [Architecture Technique](#architecture-technique)
4. [M√©thodologie de Classification](#m√©thodologie-de-classification)
5. [Installation](#installation)
6. [Utilisation](#utilisation)
7. [R√©sultats et M√©triques](#r√©sultats-et-m√©triques)
8. [Documentation Technique](#documentation-technique)
9. [Contributions](#contributions)
10. [Licence](#licence)

---

## [OVERVIEW] Vue d'ensemble du Projet

**FreeMobilaChat** est une plateforme intelligente d'analyse de sentiment con√ßue sp√©cifiquement pour l'industrie des t√©l√©communications. Ce projet de m√©moire de Master d√©montre l'application de techniques avanc√©es de Traitement du Langage Naturel (NLP) et d'Apprentissage Automatique (ML) pour analyser les retours clients provenant des interactions sur les r√©seaux sociaux.

### Objectifs Principaux

- **Classification Multi-Dimensionnelle** : Analyse de 7 dimensions (sentiment, r√©clamations, urgence, th√®mes, incidents, responsable, confiance)
- **Architecture Multi-Mod√®les** : Combinaison intelligente de BERT, Mistral AI et r√®gles m√©tier
- **Performance Optimale** : Traitement de 100+ tweets/seconde avec pr√©cision de 85-95%
- **Interface Interactive** : Dashboard temps r√©el avec visualisations interactives
- **Production-Ready** : D√©ploiement sur Streamlit Cloud avec authentification et gestion des r√¥les

### R√©sultats Cl√©s

- **Pr√©cision** : 85-95% selon les t√¢ches de classification
- **Vitesse de Traitement** : 100+ tweets/seconde
- **Analyse Multi-Dimensionnelle** : 7 dimensions de classification simultan√©es
- **Dashboard Temps R√©el** : Visualisation interactive des KPIs
- **Pr√™t pour la Production** : D√©ploy√© sur Streamlit Cloud

---

## üî¨ Contexte et Probl√©matique

### Contexte Industriel

L'industrie des t√©l√©communications g√©n√®re quotidiennement des milliers d'interactions clients sur les r√©seaux sociaux. L'analyse manuelle de ces donn√©es est co√ªteuse, lente et sujette √† des erreurs. Il existe un besoin critique d'automatiser l'analyse de sentiment et la classification des r√©clamations pour am√©liorer la r√©activit√© du service client.

### Probl√©matique de Recherche

Comment d√©velopper un syst√®me de classification automatique multi-mod√®les capable de :
1. Analyser efficacement les tweets clients avec une pr√©cision √©lev√©e
2. Classifier selon plusieurs dimensions simultan√©ment (sentiment, urgence, th√®me, etc.)
3. S'adapter aux diff√©rents besoins de performance (rapide vs pr√©cis)
4. Fournir des insights actionnables pour les √©quipes de service client

### Contribution Scientifique

Ce projet contribue √† la recherche en NLP appliqu√©e en d√©montrant :
- L'efficacit√© d'une architecture hybride combinant mod√®les pr√©-entra√Æn√©s (BERT), LLMs (Mistral) et r√®gles m√©tier
- L'optimisation des performances pour le traitement en temps r√©el
- L'application pratique de l'IA g√©n√©rative pour la classification de texte

---

## [ARCHITECTURE] Architecture Technique

### Vue d'Ensemble de l'Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    COUCHE PR√âSENTATION                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Streamlit Frontend (app.py)                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Authentification & Gestion des R√¥les              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Interface Utilisateur Interactive                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Visualisations Temps R√©el (Plotly)                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  COUCHE TRAITEMENT                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Multi-Model Orchestrator                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ TweetCleaner (Nettoyage & Pr√©processing)       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ BERTClassifier (Sentiment Rapide)               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ RuleClassifier (R√®gles M√©tier)                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ‚îÄ MistralClassifier (Analyse Contextuelle)        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ GeminiClassifier (Alternative Cloud)            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Enhanced KPIs Visualizations                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Calcul des M√©triques Business                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - G√©n√©ration de Rapports                             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    COUCHE DONN√âES                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Datasets d'Entra√Ænement (3,500+ tweets labellis√©s) ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Mod√®les Pr√©-entra√Æn√©s (BERT, Mistral)              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Cache & Optimisations                              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Diagramme de Classes UML

```mermaid
classDiagram
    class AuthService {
        +init_session_state()
        +login(email, password)
        +signup(email, name, password, role)
        +logout()
        +is_authenticated() bool
        +get_current_user() User
        +get_role_display_name(role) str
    }
    
    class MultiModelOrchestrator {
        -mode: str
        -provider: str
        -bert: BERTClassifier
        -rules: RuleClassifier
        -mistral: MistralClassifier
        +classify_batch(texts, mode) List[Dict]
        +_combine_results(bert_results, rule_results, mistral_results) Dict
    }
    
    class BERTClassifier {
        -model: AutoModel
        -tokenizer: AutoTokenizer
        +classify_batch(texts) List[Dict]
        +_predict_sentiment(text) str
    }
    
    class MistralClassifier {
        -model_name: str
        -ollama_client: Ollama
        +classify_batch(texts) List[Dict]
        +_call_mistral_api(prompt) str
    }
    
    class RuleClassifier {
        -reclamation_keywords: List[str]
        -urgence_keywords: Dict
        +classify_batch(texts) List[Dict]
        +_detect_reclamation(text) bool
        +_assess_urgency(text) str
    }
    
    class TweetCleaner {
        +clean_batch(texts) List[str]
        +remove_urls(text) str
        +normalize_whitespace(text) str
    }
    
    class EnhancedKPIsVisualizations {
        +compute_business_kpis(df) Dict
        +render_business_kpis(kpis) None
        +generate_report(df) Dict
    }
    
    class RoleManager {
        +initialize_role_system()
        +get_current_role() Role
        +check_permission(permission) bool
    }
    
    AuthService --> RoleManager : uses
    MultiModelOrchestrator --> BERTClassifier : uses
    MultiModelOrchestrator --> RuleClassifier : uses
    MultiModelOrchestrator --> MistralClassifier : uses
    MultiModelOrchestrator --> TweetCleaner : uses
    EnhancedKPIsVisualizations --> MultiModelOrchestrator : analyzes
```

### Diagramme de S√©quence - Flux de Classification

```mermaid
sequenceDiagram
    participant User as Utilisateur
    participant UI as Interface Streamlit
    participant Orchestrator as MultiModelOrchestrator
    participant Cleaner as TweetCleaner
    participant BERT as BERTClassifier
    participant Rules as RuleClassifier
    participant Mistral as MistralClassifier
    participant KPIs as EnhancedKPIsVisualizations
    
    User->>UI: Upload fichier CSV
    UI->>Cleaner: Nettoyer les tweets
    Cleaner-->>UI: Tweets nettoy√©s
    
    User->>UI: Lancer classification (mode BALANCED)
    UI->>Orchestrator: classify_batch(texts, mode='balanced')
    
    par Traitement Parall√®le
        Orchestrator->>BERT: classify_batch(texts)
        BERT-->>Orchestrator: R√©sultats sentiment
    and
        Orchestrator->>Rules: classify_batch(texts)
        Rules-->>Orchestrator: R√©sultats r√©clamations & urgence
    and
        Orchestrator->>Mistral: classify_batch(√©chantillon 20%)
        Mistral-->>Orchestrator: R√©sultats th√®mes & incidents
    end
    
    Orchestrator->>Orchestrator: Combiner r√©sultats
    Orchestrator-->>UI: R√©sultats complets
    
    UI->>KPIs: Calculer m√©triques business
    KPIs-->>UI: KPIs & Visualisations
    
    UI-->>User: Afficher dashboard interactif
```

### Diagramme de D√©ploiement

```mermaid
graph TB
    subgraph "Client Browser"
        Browser[üåê Navigateur Web]
    end
    
    subgraph "Streamlit Cloud / Local Server"
        Streamlit[üì± Streamlit App<br/>app.py]
        Pages[üìÑ Pages<br/>Classification_Mistral.py]
    end
    
    subgraph "Services Backend"
        Auth[üîê AuthService]
        Orchestrator[‚öôÔ∏è MultiModelOrchestrator]
        Cleaner[üßπ TweetCleaner]
    end
    
    subgraph "Mod√®les de Classification"
        BERT[ü§ñ BERT<br/>Hugging Face]
        Mistral[üß† Mistral AI<br/>Ollama Local]
        Rules[üìã RuleClassifier]
        Gemini[‚òÅÔ∏è Gemini API<br/>Google Cloud]
    end
    
    subgraph "Stockage & Cache"
        Cache[üíæ Cache<br/>Classification Results]
        Models[üì¶ Mod√®les<br/>Pr√©-entra√Æn√©s]
    end
    
    Browser --> Streamlit
    Streamlit --> Pages
    Pages --> Auth
    Pages --> Orchestrator
    Orchestrator --> Cleaner
    Orchestrator --> BERT
    Orchestrator --> Mistral
    Orchestrator --> Rules
    Orchestrator --> Gemini
    BERT --> Models
    Mistral --> Models
    Orchestrator --> Cache
```

### Diagramme de Cas d'Utilisation

```mermaid
graph LR
    subgraph "Acteurs"
        Client[üë§ Client SAV]
        Agent[üéß Agent SAV]
        Analyst[üìä Data Analyst]
        Manager[üëî Manager]
    end
    
    subgraph "Cas d'Utilisation"
        UC1[üì§ Upload Donn√©es]
        UC2[üßπ Nettoyer Donn√©es]
        UC3[ü§ñ Classifier Tweets]
        UC4[üìä Visualiser KPIs]
        UC5[üíæ Exporter R√©sultats]
        UC6[‚öôÔ∏è Configurer Mod√®les]
        UC7[üë• G√©rer √âquipe]
    end
    
    Client --> UC1
    Client --> UC2
    Client --> UC3
    Client --> UC4
    Client --> UC5
    
    Agent --> UC1
    Agent --> UC2
    Agent --> UC3
    Agent --> UC4
    Agent --> UC5
    
    Analyst --> UC1
    Analyst --> UC2
    Analyst --> UC3
    Analyst --> UC4
    Analyst --> UC5
    Analyst --> UC6
    
    Manager --> UC1
    Manager --> UC2
    Manager --> UC3
    Manager --> UC4
    Manager --> UC5
    Manager --> UC6
    Manager --> UC7
```

---

## [METHODOLOGY] M√©thodologie de Classification

### Architecture Multi-Mod√®les

Le syst√®me utilise une approche hybride combinant trois types de classificateurs :

#### 1. BERT (Bidirectional Encoder Representations from Transformers)
- **R√¥le** : Classification rapide du sentiment
- **Mod√®le** : `bert-base-multilingual-cased` (Hugging Face)
- **Performance** : 88% de pr√©cision, 50-100 tweets/seconde
- **Utilisation** : Traitement de 100% des tweets pour le sentiment

#### 2. Mistral AI (Large Language Model)
- **R√¥le** : Analyse contextuelle approfondie (th√®mes, incidents)
- **Mod√®le** : Mistral via Ollama (local) ou Gemini API (cloud)
- **Performance** : 92% de pr√©cision, 5-10 tweets/seconde
- **Utilisation** : Traitement d'un √©chantillon stratifi√© (20% en mode BALANCED)

#### 3. Rule-Based Classifier
- **R√¥le** : D√©tection rapide des r√©clamations et √©valuation de l'urgence
- **M√©thode** : R√®gles m√©tier bas√©es sur mots-cl√©s et patterns
- **Performance** : 78% de pr√©cision, 1000+ tweets/seconde
- **Utilisation** : Traitement de 100% des tweets pour r√©clamations/urgence

### Modes de Performance

| Mode | Mod√®les Utilis√©s | Pr√©cision | Temps (5000 tweets) | Cas d'Usage |
|------|------------------|-----------|---------------------|-------------|
| **RAPIDE** | BERT + R√®gles | 75% | ~20s | Tests rapides, d√©monstrations |
| **√âQUILIBR√â** | BERT + R√®gles + Mistral (20%) | 88% | ~2min | Production recommand√©e |
| **PR√âCIS** | BERT + Mistral (100%) | 95% | ~10min | Analyses critiques, rapports d√©taill√©s |

### Dimensions de Classification

Le syst√®me classifie chaque tweet selon 7 dimensions :

1. **Sentiment** : POSITIF, NEUTRE, NEGATIF
2. **R√©clamation** : OUI, NON
3. **Urgence** : FAIBLE, MOYENNE, ELEVEE, CRITIQUE
4. **Th√®me** : FIBRE, MOBILE, TV, FACTURE, SAV, RESEAU, AUTRE
5. **Type d'Incident** : PANNE, LENTEUR, FACTURATION, PROCESSUS_SAV, INFO, AUTRE
6. **Responsable** : TECHNIQUE, COMMERCIAL, RESEAU, AUTRE
7. **Confiance** : Score de 0.0 √† 1.0

---

## [SETUP] Installation

### Pr√©requis

- **Syst√®me d'exploitation** : Windows 10/11, macOS, ou Linux
- **Python** : Version 3.11 ou sup√©rieure
- **RAM** : Minimum 8GB (16GB recommand√© pour BERT)
- **Espace disque** : 2GB pour les d√©pendances et mod√®les
- **Internet** : Requis pour le t√©l√©chargement des mod√®les
- **Ollama** (optionnel) : Pour Mistral local - [Installation Ollama](https://ollama.ai)

### Installation Locale

#### √âtape 1 : Cloner le Repository

```bash
git clone https://github.com/Archimedh-Anderson/FreeMobileApp.git
cd FreeMobileApp
```

#### √âtape 2 : Cr√©er l'Environnement Virtuel

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

#### √âtape 3 : Installer les D√©pendances

```bash
# Pour la production
pip install -r requirements-streamlit.txt

# Pour le d√©veloppement complet
pip install -r requirements.txt
pip install -r requirements.dev.txt
```

#### √âtape 4 : T√©l√©charger les Mod√®les Pr√©-entra√Æn√©s (Optionnel)

```bash
# Pour le classificateur BERT
python -c "from transformers import AutoModel; AutoModel.from_pretrained('bert-base-multilingual-cased')"

# Pour Mistral LLM (n√©cessite Ollama)
ollama pull mistral
```

#### √âtape 5 : Configuration de l'Environnement

Cr√©ez un fichier `.env` √† la racine du projet avec les variables essentielles :

```env
# Configuration Gemini API (Recommand√© pour classification cloud)
GEMINI_API_KEY=your_gemini_api_key_here
# Obtenez votre cl√© sur: https://makersuite.google.com/app/apikey

# Configuration Mistral/Ollama (Optionnel - pour classification locale)
OLLAMA_BASE_URL=http://localhost:11434
MISTRAL_MODEL=mistral:latest
# Installation: https://ollama.ai puis "ollama pull mistral"

# Configuration Application
ENVIRONMENT=development
DEBUG=false
LOG_LEVEL=INFO
```

**Note importante** : Le fichier `.env` est optionnel pour le d√©veloppement local mais requis pour certaines fonctionnalit√©s (Gemini API, etc.). Ne commitez jamais le fichier `.env` (il est dans `.gitignore`).

#### √âtape 6 : Lancer l'Application

```bash
streamlit run streamlit_app/app.py
```

L'application s'ouvrira automatiquement dans votre navigateur √† `http://localhost:8503`

### D√©ploiement Rapide

#### D√©marrage Local (Windows)
```cmd
start_application.bat
```

#### D√©marrage Local (Linux/macOS)
```bash
chmod +x start_application.sh
./start_application.sh
```

#### D√©ploiement Streamlit Cloud

1. Forkez le repository sur GitHub
2. Visitez [Streamlit Cloud](https://streamlit.io/cloud)
3. Connectez-vous avec votre compte GitHub
4. Cr√©ez une nouvelle app avec :
   - **Main file** : `streamlit_app/app.py`
   - **Python version** : 3.10+
   - **Requirements** : `requirements.production.txt` ou `streamlit_app/requirements.txt`
5. Ajoutez les secrets dans les param√®tres (GEMINI_API_KEY, etc.)
6. Cliquez sur "Deploy"

---

## üíª Utilisation

### D√©marrage Rapide

1. **Lancer l'application** :
   ```bash
   streamlit run streamlit_app/app.py
   ```

2. **S'authentifier** :
   - Cr√©ez un compte ou connectez-vous
   - S√©lectionnez votre r√¥le (Client SAV, Agent SAV, Data Analyst, Manager)

3. **Acc√©der √† la Classification** :
   - Cliquez sur "Start Now" ou "Start Mistral Classification"
   - Vous serez redirig√© vers la page de classification

### Workflow de Classification

#### √âtape 1 : Upload & Nettoyage

1. **Upload du fichier** :
   - Format : CSV
   - Colonne requise : `text` (ou s√©lectionnez une colonne de texte)
   - Taille maximale : 500 MB

2. **Nettoyage automatique** :
   - Suppression des URLs
   - Normalisation des espaces
   - Gestion des emojis
   - D√©tection automatique de l'encodage

#### √âtape 2 : Classification Intelligente

1. **S√©lection du mod√®le** :
   - **Mistral (Local)** : Via Ollama (recommand√© pour performance)
   - **Gemini API (Externe)** : Via Google Cloud API

2. **Choix du mode** :
   - **RAPIDE (20s)** : BERT + R√®gles - 75% pr√©cision
   - **√âQUILIBR√â (2min)** : BERT + R√®gles + Mistral (20%) - 88% pr√©cision ‚≠ê Recommand√©
   - **PR√âCIS (10min)** : BERT + Mistral (100%) - 95% pr√©cision

3. **Lancement** :
   - Cliquez sur "D√©marrer la Classification Intelligente"
   - Suivez la progression en temps r√©el

#### √âtape 3 : R√©sultats & Export

1. **Visualisation des KPIs** :
   - Indicateurs cl√©s de performance
   - Graphiques interactifs (Plotly)
   - Tableaux d√©taill√©s

2. **Export des r√©sultats** :
   - **CSV** : Donn√©es classifi√©es compl√®tes
   - **JSON** : M√©triques et KPIs
   - **Excel** : Rapport multi-feuilles

### Exemple d'Utilisation

```python
# Exemple de fichier CSV d'entr√©e
text
"Mon internet ne fonctionne plus depuis ce matin, tr√®s m√©content"
"Super service client, merci beaucoup pour votre aide!"
"J'ai un probl√®me avec ma facture, pouvez-vous m'aider?"
```

Apr√®s classification, vous obtiendrez :

| text | sentiment | is_claim | urgence | topics | incident | confidence |
|------|-----------|----------|---------|--------|----------|------------|
| "Mon internet..." | NEGATIF | OUI | ELEVEE | RESEAU | PANNE | 0.92 |
| "Super service..." | POSITIF | NON | FAIBLE | SAV | INFO | 0.88 |
| "J'ai un probl√®me..." | NEUTRE | OUI | MOYENNE | FACTURE | FACTURATION | 0.85 |

---

## üìä R√©sultats et M√©triques

### Performance des Mod√®les

| Mod√®le | Pr√©cision | Rappel | F1-Score | Vitesse (tweets/sec) |
|--------|-----------|--------|----------|---------------------|
| **Mistral LLM** | 92% | 0.90 | 0.91 | 5-10 |
| **BERT Fine-tuned** | 88% | 0.86 | 0.87 | 50-100 |
| **Rule-Based** | 78% | 0.73 | 0.74 | 1000+ |
| **Multi-Model (BALANCED)** | 88% | 0.87 | 0.88 | 25-50 |

### Performance par Dimension

- **Classification de Sentiment** : 90% de pr√©cision, F1-Score: 0.89
- **D√©tection de R√©clamations** : 87% de pr√©cision, Pr√©cision: 0.88
- **√âvaluation de l'Urgence** : 85% de pr√©cision
- **Cat√©gorisation des Th√®mes** : 91% de pr√©cision, Top-3 pr√©cision: 97%

### Indicateurs Cl√©s de Performance (KPIs)

Le syst√®me calcule automatiquement 10+ KPIs business :

1. **Taux de R√©clamations** : Pourcentage de tweets identifi√©s comme r√©clamations
2. **Taux de Sentiment N√©gatif** : Pourcentage de tweets avec sentiment n√©gatif
3. **Taux d'Urgence √âlev√©e** : Pourcentage de tweets n√©cessitant une action urgente
4. **Score de Confiance Moyen** : Confiance moyenne des classifications
5. **Distribution des Th√®mes** : R√©partition par cat√©gorie (FIBRE, MOBILE, etc.)
6. **Types d'Incidents** : Distribution des types d'incidents d√©tect√©s
7. **Temps de Traitement** : Performance du syst√®me
8. **Taux de Succ√®s** : Pourcentage de tweets classifi√©s avec succ√®s
9. **Volume Trait√©** : Nombre total de tweets analys√©s
10. **Tendances Temporelles** : √âvolution dans le temps (si donn√©es temporelles disponibles)

---

## üîß Documentation Technique

### Structure du Projet

```
FreeMobilaChat/
‚îÇ
‚îú‚îÄ‚îÄ streamlit_app/              # Application principale
‚îÇ   ‚îú‚îÄ‚îÄ app.py                  # Point d'entr√©e principal
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration centralis√©e
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pages/                  # Pages de l'application
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Classification_Mistral.py  # Page de classification
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ components/             # Composants UI r√©utilisables
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth_forms.py       # Formulaires d'authentification
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ charts.py           # Composants de visualisation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/               # Logique m√©tier
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth_service.py     # Service d'authentification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mistral_classifier.py      # Classificateur Mistral
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bert_classifier.py         # Classificateur BERT
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rule_classifier.py        # Classificateur par r√®gles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_model_orchestrator.py  # Orchestrateur multi-mod√®les
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tweet_cleaner.py           # Nettoyage de tweets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_classifier.py       # Classificateur Gemini
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enhanced_kpis_vizualizations.py  # KPIs avanc√©s
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ultra_optimized_classifier.py   # Classificateur optimis√©
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ role_manager.py            # Gestion des r√¥les
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                  # Fonctions utilitaires
‚îÇ       ‚îú‚îÄ‚îÄ helpers.py
‚îÇ       ‚îî‚îÄ‚îÄ validators.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # Scripts utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.sh           # Script de tests
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Datasets
‚îÇ   ‚îú‚îÄ‚îÄ training/              # Donn√©es d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ validation/            # Donn√©es de validation
‚îÇ   ‚îî‚îÄ‚îÄ test/                  # Donn√©es de test
‚îÇ
‚îú‚îÄ‚îÄ models/                     # Mod√®les entra√Æn√©s
‚îÇ   ‚îú‚îÄ‚îÄ bert_finetuned/        # BERT fine-tun√©
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/           # Embeddings cach√©s
‚îÇ
‚îú‚îÄ‚îÄ tests/                      # Tests
‚îÇ   ‚îú‚îÄ‚îÄ unit/                  # Tests unitaires
‚îÇ   ‚îú‚îÄ‚îÄ integration/           # Tests d'int√©gration
‚îÇ   ‚îî‚îÄ‚îÄ e2e/                   # Tests end-to-end
‚îÇ
‚îú‚îÄ‚îÄ requirements-streamlit.txt  # D√©pendances production
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances compl√®tes
‚îú‚îÄ‚îÄ README.md                   # Ce fichier
‚îî‚îÄ‚îÄ LICENSE                     # Licence MIT
```

### Stack Technologique

#### Framework Core
- **Python** : 3.11+
- **Streamlit** : 1.51.0 (Framework web interactif)

#### Machine Learning
- **Transformers** : 4.44.2 (Hugging Face) - Mod√®les BERT
- **PyTorch** : 2.4.1 - Backend pour Transformers
- **Sentence-Transformers** : 3.1.1 - Embeddings s√©mantiques
- **Scikit-learn** : 1.5.2 - Outils ML compl√©mentaires

#### NLP & Language Models
- **Mistral AI** : Via Ollama (mod√®les locaux)
- **BERT-base-multilingual** : Mod√®le pr√©-entra√Æn√© Hugging Face
- **Gemini API** : Google Generative AI (alternative cloud)
- **spaCy** : 3.8.2 - Traitement NLP suppl√©mentaire

#### Traitement de Donn√©es
- **Pandas** : 2.2.3 - Manipulation de donn√©es
- **NumPy** : 2.1.1 - Calculs num√©riques

#### Visualisation
- **Plotly** : 5.24.1 - Graphiques interactifs

#### Authentification & S√©curit√©
- **bcrypt** : 4.0.1 - Hachage de mots de passe
- **PyJWT** : 2.8.0 - Tokens JWT

---

## üß™ Tests

### Ex√©cution des Tests

```bash
# Tous les tests
./scripts/run_tests.sh all

# Tests unitaires uniquement
./scripts/run_tests.sh unit

# Tests d'int√©gration (n√©cessite GEMINI_API_KEY)
./scripts/run_tests.sh integration

# Tests avec couverture
pytest tests/ --cov=streamlit_app --cov-report=html
```

### √âvaluation des Performances

```bash
# √âvaluer sur dataset de test
python scripts/evaluate_model.py --dataset tests/data/test_dataset.csv

# G√©n√©ration de rapport HTML
python scripts/generate_report.py
```

---

## ‚öôÔ∏è Configuration

### Variables d'Environnement

Cr√©ez un fichier `.env` √† la racine du projet :

```env
# Configuration Mistral/Ollama
OLLAMA_BASE_URL=http://localhost:11434
MISTRAL_MODEL=mistral:latest

# Configuration Gemini API (Optionnel)
GEMINI_API_KEY=your_gemini_api_key_here

# Configuration Application
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=INFO
BACKEND_URL=http://localhost:8000
```

### Configuration Streamlit

Le fichier `.streamlit/config.toml` contient :

```toml
[server]
port = 8503
enableCORS = false
enableXsrfProtection = true
maxUploadSize = 200

[theme]
primaryColor = "#667eea"
backgroundColor = "#ffffff"
secondaryBackgroundColor = "#f0f2f6"
```

---

## ü§ù Contributions

Les contributions sont les bienvenues ! N'h√©sitez pas √† soumettre une Pull Request.

1. Forkez le repository
2. Cr√©ez votre branche de fonctionnalit√© (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Poussez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

---

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

### Usage Acad√©mique

Ce projet est soumis dans le cadre d'un m√©moire de Master. Si vous utilisez ce travail dans une recherche acad√©mique, veuillez citer :

```bibtex
@mastersthesis{archimed2025freemobilachat,
  title={Multi-Model Sentiment Analysis for Telecommunications Customer Service},
  author={Archimed, Anderson},
  year={2025},
  school={[Votre Universit√©]},
  type={Master's Thesis}
}
```

---

## üìû Contact & Support

**Auteur** : Anderson Archimed  
**GitHub** : [@Archimedh-Anderson](https://github.com/Archimedh-Anderson)  
**Repository** : https://github.com/Archimedh-Anderson/FreeMobileApp  
**D√©mo Live** : https://freemobilachat.streamlit.app

Pour les questions acad√©miques ou opportunit√©s de collaboration, contactez via GitHub.

---

## üôè Remerciements

Remerciements sp√©ciaux √† :
- **Hugging Face** pour les mod√®les transformer et l'infrastructure
- **Streamlit** pour l'excellent framework d'application
- **Mistral AI** pour les puissants mod√®les de langage
- **Communaut√© Open Source** pour les outils et biblioth√®ques inestimables

---

<div align="center">

**Derni√®re Mise √† Jour** : Janvier 2025  
**Version** : 2.0.0  
**Statut** : Production Ready - Soumission Acad√©mique

Fait avec ‚ù§Ô∏è pour la communaut√© NLP/ML

</div>
