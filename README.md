# FreeMobilaChat - Plateforme d'Analyse de Sentiment Multi-ModÃ¨les

<div align="center">

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.51.0-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)

**Application de classification automatique de rÃ©clamations avec Intelligence Artificielle Multi-ModÃ¨les**

*MÃ©moire de Master en Data Science | Classification Multi-KPI | Architecture Production-Ready*

[Installation](#-installation) â€¢ [Architecture](#-architecture-technique) â€¢ [Utilisation](#-utilisation) â€¢ [Documentation](#-documentation-technique)

</div>

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble du Projet](#-vue-densemble-du-projet)
2. [Contexte et ProblÃ©matique](#-contexte-et-problÃ©matique)
3. [Architecture Technique](#-architecture-technique)
4. [MÃ©thodologie de Classification](#-mÃ©thodologie-de-classification)
5. [Installation](#-installation)
6. [Utilisation](#-utilisation)
7. [RÃ©sultats et MÃ©triques](#-rÃ©sultats-et-mÃ©triques)
8. [Documentation Technique](#-documentation-technique)
9. [Contributions](#-contributions)
10. [Licence](#-licence)

---

## ğŸ¯ Vue d'ensemble du Projet

**FreeMobilaChat** est une plateforme intelligente d'analyse de sentiment conÃ§ue spÃ©cifiquement pour l'industrie des tÃ©lÃ©communications. Ce projet de mÃ©moire de Master dÃ©montre l'application de techniques avancÃ©es de Traitement du Langage Naturel (NLP) et d'Apprentissage Automatique (ML) pour analyser les retours clients provenant des interactions sur les rÃ©seaux sociaux.

### Objectifs Principaux

- **Classification Multi-Dimensionnelle** : Analyse de 7 dimensions (sentiment, rÃ©clamations, urgence, thÃ¨mes, incidents, responsable, confiance)
- **Architecture Multi-ModÃ¨les** : Combinaison intelligente de BERT, Mistral AI et rÃ¨gles mÃ©tier
- **Performance Optimale** : Traitement de 100+ tweets/seconde avec prÃ©cision de 85-95%
- **Interface Interactive** : Dashboard temps rÃ©el avec visualisations interactives
- **Production-Ready** : DÃ©ploiement sur Streamlit Cloud avec authentification et gestion des rÃ´les

### RÃ©sultats ClÃ©s

- **PrÃ©cision** : 85-95% selon les tÃ¢ches de classification
- **Vitesse de Traitement** : 100+ tweets/seconde
- **Analyse Multi-Dimensionnelle** : 7 dimensions de classification simultanÃ©es
- **Dashboard Temps RÃ©el** : Visualisation interactive des KPIs
- **PrÃªt pour la Production** : DÃ©ployÃ© sur Streamlit Cloud

---

## ğŸ”¬ Contexte et ProblÃ©matique

### Contexte Industriel

L'industrie des tÃ©lÃ©communications gÃ©nÃ¨re quotidiennement des milliers d'interactions clients sur les rÃ©seaux sociaux. L'analyse manuelle de ces donnÃ©es est coÃ»teuse, lente et sujette Ã  des erreurs. Il existe un besoin critique d'automatiser l'analyse de sentiment et la classification des rÃ©clamations pour amÃ©liorer la rÃ©activitÃ© du service client.

### ProblÃ©matique de Recherche

Comment dÃ©velopper un systÃ¨me de classification automatique multi-modÃ¨les capable de :
1. Analyser efficacement les tweets clients avec une prÃ©cision Ã©levÃ©e
2. Classifier selon plusieurs dimensions simultanÃ©ment (sentiment, urgence, thÃ¨me, etc.)
3. S'adapter aux diffÃ©rents besoins de performance (rapide vs prÃ©cis)
4. Fournir des insights actionnables pour les Ã©quipes de service client

### Contribution Scientifique

Ce projet contribue Ã  la recherche en NLP appliquÃ©e en dÃ©montrant :
- L'efficacitÃ© d'une architecture hybride combinant modÃ¨les prÃ©-entraÃ®nÃ©s (BERT), LLMs (Mistral) et rÃ¨gles mÃ©tier
- L'optimisation des performances pour le traitement en temps rÃ©el
- L'application pratique de l'IA gÃ©nÃ©rative pour la classification de texte

---

## ğŸ—ï¸ Architecture Technique

### Vue d'Ensemble de l'Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COUCHE PRÃ‰SENTATION                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Streamlit Frontend (app.py)                         â”‚  â”‚
â”‚  â”‚  - Authentification & Gestion des RÃ´les              â”‚  â”‚
â”‚  â”‚  - Interface Utilisateur Interactive                 â”‚  â”‚
â”‚  â”‚  - Visualisations Temps RÃ©el (Plotly)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  COUCHE TRAITEMENT                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Multi-Model Orchestrator                            â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ TweetCleaner (Nettoyage & PrÃ©processing)       â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ BERTClassifier (Sentiment Rapide)               â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ RuleClassifier (RÃ¨gles MÃ©tier)                  â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€ MistralClassifier (Analyse Contextuelle)        â”‚  â”‚
â”‚  â”‚  â””â”€â”€ GeminiClassifier (Alternative Cloud)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Enhanced KPIs Visualizations                        â”‚  â”‚
â”‚  â”‚  - Calcul des MÃ©triques Business                     â”‚  â”‚
â”‚  â”‚  - GÃ©nÃ©ration de Rapports                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COUCHE DONNÃ‰ES                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Datasets d'EntraÃ®nement (3,500+ tweets labellisÃ©s) â”‚  â”‚
â”‚  â”‚  ModÃ¨les PrÃ©-entraÃ®nÃ©s (BERT, Mistral)              â”‚  â”‚
â”‚  â”‚  Cache & Optimisations                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Diagramme de Classes UML

```mermaid
classDiagram
    class AuthService {
        +init_session_state()
        +login(email, password)
        +signup(email, name, password, role)
        +logout()
        +is_authenticated() bool
        +get_current_user() User
        +get_role_display_name(role) str
    }
    
    class MultiModelOrchestrator {
        -mode: str
        -provider: str
        -bert: BERTClassifier
        -rules: RuleClassifier
        -mistral: MistralClassifier
        +classify_batch(texts, mode) List[Dict]
        +_combine_results(bert_results, rule_results, mistral_results) Dict
    }
    
    class BERTClassifier {
        -model: AutoModel
        -tokenizer: AutoTokenizer
        +classify_batch(texts) List[Dict]
        +_predict_sentiment(text) str
    }
    
    class MistralClassifier {
        -model_name: str
        -ollama_client: Ollama
        +classify_batch(texts) List[Dict]
        +_call_mistral_api(prompt) str
    }
    
    class RuleClassifier {
        -reclamation_keywords: List[str]
        -urgence_keywords: Dict
        +classify_batch(texts) List[Dict]
        +_detect_reclamation(text) bool
        +_assess_urgency(text) str
    }
    
    class TweetCleaner {
        +clean_batch(texts) List[str]
        +remove_urls(text) str
        +normalize_whitespace(text) str
    }
    
    class EnhancedKPIsVisualizations {
        +compute_business_kpis(df) Dict
        +render_business_kpis(kpis) None
        +generate_report(df) Dict
    }
    
    class RoleManager {
        +initialize_role_system()
        +get_current_role() Role
        +check_permission(permission) bool
    }
    
    AuthService --> RoleManager : uses
    MultiModelOrchestrator --> BERTClassifier : uses
    MultiModelOrchestrator --> RuleClassifier : uses
    MultiModelOrchestrator --> MistralClassifier : uses
    MultiModelOrchestrator --> TweetCleaner : uses
    EnhancedKPIsVisualizations --> MultiModelOrchestrator : analyzes
```

### Diagramme de SÃ©quence - Flux de Classification

```mermaid
sequenceDiagram
    participant User as Utilisateur
    participant UI as Interface Streamlit
    participant Orchestrator as MultiModelOrchestrator
    participant Cleaner as TweetCleaner
    participant BERT as BERTClassifier
    participant Rules as RuleClassifier
    participant Mistral as MistralClassifier
    participant KPIs as EnhancedKPIsVisualizations
    
    User->>UI: Upload fichier CSV
    UI->>Cleaner: Nettoyer les tweets
    Cleaner-->>UI: Tweets nettoyÃ©s
    
    User->>UI: Lancer classification (mode BALANCED)
    UI->>Orchestrator: classify_batch(texts, mode='balanced')
    
    par Traitement ParallÃ¨le
        Orchestrator->>BERT: classify_batch(texts)
        BERT-->>Orchestrator: RÃ©sultats sentiment
    and
        Orchestrator->>Rules: classify_batch(texts)
        Rules-->>Orchestrator: RÃ©sultats rÃ©clamations & urgence
    and
        Orchestrator->>Mistral: classify_batch(Ã©chantillon 20%)
        Mistral-->>Orchestrator: RÃ©sultats thÃ¨mes & incidents
    end
    
    Orchestrator->>Orchestrator: Combiner rÃ©sultats
    Orchestrator-->>UI: RÃ©sultats complets
    
    UI->>KPIs: Calculer mÃ©triques business
    KPIs-->>UI: KPIs & Visualisations
    
    UI-->>User: Afficher dashboard interactif
```

### Diagramme de DÃ©ploiement

```mermaid
graph TB
    subgraph "Client Browser"
        Browser[ğŸŒ Navigateur Web]
    end
    
    subgraph "Streamlit Cloud / Local Server"
        Streamlit[ğŸ“± Streamlit App<br/>app.py]
        Pages[ğŸ“„ Pages<br/>Classification_Mistral.py]
    end
    
    subgraph "Services Backend"
        Auth[ğŸ” AuthService]
        Orchestrator[âš™ï¸ MultiModelOrchestrator]
        Cleaner[ğŸ§¹ TweetCleaner]
    end
    
    subgraph "ModÃ¨les de Classification"
        BERT[ğŸ¤– BERT<br/>Hugging Face]
        Mistral[ğŸ§  Mistral AI<br/>Ollama Local]
        Rules[ğŸ“‹ RuleClassifier]
        Gemini[â˜ï¸ Gemini API<br/>Google Cloud]
    end
    
    subgraph "Stockage & Cache"
        Cache[ğŸ’¾ Cache<br/>Classification Results]
        Models[ğŸ“¦ ModÃ¨les<br/>PrÃ©-entraÃ®nÃ©s]
    end
    
    Browser --> Streamlit
    Streamlit --> Pages
    Pages --> Auth
    Pages --> Orchestrator
    Orchestrator --> Cleaner
    Orchestrator --> BERT
    Orchestrator --> Mistral
    Orchestrator --> Rules
    Orchestrator --> Gemini
    BERT --> Models
    Mistral --> Models
    Orchestrator --> Cache
```

### Diagramme de Cas d'Utilisation

```mermaid
graph LR
    subgraph "Acteurs"
        Client[ğŸ‘¤ Client SAV]
        Agent[ğŸ§ Agent SAV]
        Analyst[ğŸ“Š Data Analyst]
        Manager[ğŸ‘” Manager]
    end
    
    subgraph "Cas d'Utilisation"
        UC1[ğŸ“¤ Upload DonnÃ©es]
        UC2[ğŸ§¹ Nettoyer DonnÃ©es]
        UC3[ğŸ¤– Classifier Tweets]
        UC4[ğŸ“Š Visualiser KPIs]
        UC5[ğŸ’¾ Exporter RÃ©sultats]
        UC6[âš™ï¸ Configurer ModÃ¨les]
        UC7[ğŸ‘¥ GÃ©rer Ã‰quipe]
    end
    
    Client --> UC1
    Client --> UC2
    Client --> UC3
    Client --> UC4
    Client --> UC5
    
    Agent --> UC1
    Agent --> UC2
    Agent --> UC3
    Agent --> UC4
    Agent --> UC5
    
    Analyst --> UC1
    Analyst --> UC2
    Analyst --> UC3
    Analyst --> UC4
    Analyst --> UC5
    Analyst --> UC6
    
    Manager --> UC1
    Manager --> UC2
    Manager --> UC3
    Manager --> UC4
    Manager --> UC5
    Manager --> UC6
    Manager --> UC7
```

---

## ğŸ§  MÃ©thodologie de Classification

### Architecture Multi-ModÃ¨les

Le systÃ¨me utilise une approche hybride combinant trois types de classificateurs :

#### 1. BERT (Bidirectional Encoder Representations from Transformers)
- **RÃ´le** : Classification rapide du sentiment
- **ModÃ¨le** : `bert-base-multilingual-cased` (Hugging Face)
- **Performance** : 88% de prÃ©cision, 50-100 tweets/seconde
- **Utilisation** : Traitement de 100% des tweets pour le sentiment

#### 2. Mistral AI (Large Language Model)
- **RÃ´le** : Analyse contextuelle approfondie (thÃ¨mes, incidents)
- **ModÃ¨le** : Mistral via Ollama (local) ou Gemini API (cloud)
- **Performance** : 92% de prÃ©cision, 5-10 tweets/seconde
- **Utilisation** : Traitement d'un Ã©chantillon stratifiÃ© (20% en mode BALANCED)

#### 3. Rule-Based Classifier
- **RÃ´le** : DÃ©tection rapide des rÃ©clamations et Ã©valuation de l'urgence
- **MÃ©thode** : RÃ¨gles mÃ©tier basÃ©es sur mots-clÃ©s et patterns
- **Performance** : 78% de prÃ©cision, 1000+ tweets/seconde
- **Utilisation** : Traitement de 100% des tweets pour rÃ©clamations/urgence

### Modes de Performance

| Mode | ModÃ¨les UtilisÃ©s | PrÃ©cision | Temps (5000 tweets) | Cas d'Usage |
|------|------------------|-----------|---------------------|-------------|
| **RAPIDE** | BERT + RÃ¨gles | 75% | ~20s | Tests rapides, dÃ©monstrations |
| **Ã‰QUILIBRÃ‰** | BERT + RÃ¨gles + Mistral (20%) | 88% | ~2min | Production recommandÃ©e |
| **PRÃ‰CIS** | BERT + Mistral (100%) | 95% | ~10min | Analyses critiques, rapports dÃ©taillÃ©s |

### Dimensions de Classification

Le systÃ¨me classifie chaque tweet selon 7 dimensions :

1. **Sentiment** : POSITIF, NEUTRE, NEGATIF
2. **RÃ©clamation** : OUI, NON
3. **Urgence** : FAIBLE, MOYENNE, ELEVEE, CRITIQUE
4. **ThÃ¨me** : FIBRE, MOBILE, TV, FACTURE, SAV, RESEAU, AUTRE
5. **Type d'Incident** : PANNE, LENTEUR, FACTURATION, PROCESSUS_SAV, INFO, AUTRE
6. **Responsable** : TECHNIQUE, COMMERCIAL, RESEAU, AUTRE
7. **Confiance** : Score de 0.0 Ã  1.0

---

## ğŸ“¦ Installation

### PrÃ©requis

- **SystÃ¨me d'exploitation** : Windows 10/11, macOS, ou Linux
- **Python** : Version 3.11 ou supÃ©rieure
- **RAM** : Minimum 8GB (16GB recommandÃ© pour BERT)
- **Espace disque** : 2GB pour les dÃ©pendances et modÃ¨les
- **Internet** : Requis pour le tÃ©lÃ©chargement des modÃ¨les
- **Ollama** (optionnel) : Pour Mistral local - [Installation Ollama](https://ollama.ai)

### Installation Locale

#### Ã‰tape 1 : Cloner le Repository

```bash
git clone https://github.com/Archimedh-Anderson/FreeMobileApp.git
cd FreeMobileApp
```

#### Ã‰tape 2 : CrÃ©er l'Environnement Virtuel

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

#### Ã‰tape 3 : Installer les DÃ©pendances

```bash
# Pour la production
pip install -r requirements-streamlit.txt

# Pour le dÃ©veloppement complet
pip install -r requirements.txt
pip install -r requirements.dev.txt
```

#### Ã‰tape 4 : TÃ©lÃ©charger les ModÃ¨les PrÃ©-entraÃ®nÃ©s (Optionnel)

```bash
# Pour le classificateur BERT
python -c "from transformers import AutoModel; AutoModel.from_pretrained('bert-base-multilingual-cased')"

# Pour Mistral LLM (nÃ©cessite Ollama)
ollama pull mistral
```

#### Ã‰tape 5 : Configuration de l'Environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet :

```env
# Configuration Mistral/Ollama
OLLAMA_BASE_URL=http://localhost:11434
MISTRAL_MODEL=mistral:latest

# Configuration Gemini API (Optionnel)
GEMINI_API_KEY=your_gemini_api_key_here

# Configuration Application
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=INFO
BACKEND_URL=http://localhost:8000
```

#### Ã‰tape 6 : Lancer l'Application

```bash
streamlit run streamlit_app/app.py
```

L'application s'ouvrira automatiquement dans votre navigateur Ã  `http://localhost:8503`

### DÃ©ploiement Cloud (Streamlit Cloud)

1. Forkez le repository sur GitHub
2. Visitez [Streamlit Cloud](https://streamlit.io/cloud)
3. Connectez-vous avec votre compte GitHub
4. Cliquez sur "New app"
5. SÃ©lectionnez votre repository forkÃ©
6. DÃ©finissez le fichier principal : `streamlit_app/app.py`
7. DÃ©finissez la version Python : 3.11
8. Ajoutez les secrets (GEMINI_API_KEY, etc.) dans les paramÃ¨tres
9. Cliquez sur "Deploy"

---

## ğŸ’» Utilisation

### DÃ©marrage Rapide

1. **Lancer l'application** :
   ```bash
   streamlit run streamlit_app/app.py
   ```

2. **S'authentifier** :
   - CrÃ©ez un compte ou connectez-vous
   - SÃ©lectionnez votre rÃ´le (Client SAV, Agent SAV, Data Analyst, Manager)

3. **AccÃ©der Ã  la Classification** :
   - Cliquez sur "Start Now" ou "Start Mistral Classification"
   - Vous serez redirigÃ© vers la page de classification

### Workflow de Classification

#### Ã‰tape 1 : Upload & Nettoyage

1. **Upload du fichier** :
   - Format : CSV
   - Colonne requise : `text` (ou sÃ©lectionnez une colonne de texte)
   - Taille maximale : 500 MB

2. **Nettoyage automatique** :
   - Suppression des URLs
   - Normalisation des espaces
   - Gestion des emojis
   - DÃ©tection automatique de l'encodage

#### Ã‰tape 2 : Classification Intelligente

1. **SÃ©lection du modÃ¨le** :
   - **Mistral (Local)** : Via Ollama (recommandÃ© pour performance)
   - **Gemini API (Externe)** : Via Google Cloud API

2. **Choix du mode** :
   - **RAPIDE (20s)** : BERT + RÃ¨gles - 75% prÃ©cision
   - **Ã‰QUILIBRÃ‰ (2min)** : BERT + RÃ¨gles + Mistral (20%) - 88% prÃ©cision â­ RecommandÃ©
   - **PRÃ‰CIS (10min)** : BERT + Mistral (100%) - 95% prÃ©cision

3. **Lancement** :
   - Cliquez sur "DÃ©marrer la Classification Intelligente"
   - Suivez la progression en temps rÃ©el

#### Ã‰tape 3 : RÃ©sultats & Export

1. **Visualisation des KPIs** :
   - Indicateurs clÃ©s de performance
   - Graphiques interactifs (Plotly)
   - Tableaux dÃ©taillÃ©s

2. **Export des rÃ©sultats** :
   - **CSV** : DonnÃ©es classifiÃ©es complÃ¨tes
   - **JSON** : MÃ©triques et KPIs
   - **Excel** : Rapport multi-feuilles

### Exemple d'Utilisation

```python
# Exemple de fichier CSV d'entrÃ©e
text
"Mon internet ne fonctionne plus depuis ce matin, trÃ¨s mÃ©content"
"Super service client, merci beaucoup pour votre aide!"
"J'ai un problÃ¨me avec ma facture, pouvez-vous m'aider?"
```

AprÃ¨s classification, vous obtiendrez :

| text | sentiment | is_claim | urgence | topics | incident | confidence |
|------|-----------|----------|---------|--------|----------|------------|
| "Mon internet..." | NEGATIF | OUI | ELEVEE | RESEAU | PANNE | 0.92 |
| "Super service..." | POSITIF | NON | FAIBLE | SAV | INFO | 0.88 |
| "J'ai un problÃ¨me..." | NEUTRE | OUI | MOYENNE | FACTURE | FACTURATION | 0.85 |

---

## ğŸ“Š RÃ©sultats et MÃ©triques

### Performance des ModÃ¨les

| ModÃ¨le | PrÃ©cision | Rappel | F1-Score | Vitesse (tweets/sec) |
|--------|-----------|--------|----------|---------------------|
| **Mistral LLM** | 92% | 0.90 | 0.91 | 5-10 |
| **BERT Fine-tuned** | 88% | 0.86 | 0.87 | 50-100 |
| **Rule-Based** | 78% | 0.73 | 0.74 | 1000+ |
| **Multi-Model (BALANCED)** | 88% | 0.87 | 0.88 | 25-50 |

### Performance par Dimension

- **Classification de Sentiment** : 90% de prÃ©cision, F1-Score: 0.89
- **DÃ©tection de RÃ©clamations** : 87% de prÃ©cision, PrÃ©cision: 0.88
- **Ã‰valuation de l'Urgence** : 85% de prÃ©cision
- **CatÃ©gorisation des ThÃ¨mes** : 91% de prÃ©cision, Top-3 prÃ©cision: 97%

### Indicateurs ClÃ©s de Performance (KPIs)

Le systÃ¨me calcule automatiquement 10+ KPIs business :

1. **Taux de RÃ©clamations** : Pourcentage de tweets identifiÃ©s comme rÃ©clamations
2. **Taux de Sentiment NÃ©gatif** : Pourcentage de tweets avec sentiment nÃ©gatif
3. **Taux d'Urgence Ã‰levÃ©e** : Pourcentage de tweets nÃ©cessitant une action urgente
4. **Score de Confiance Moyen** : Confiance moyenne des classifications
5. **Distribution des ThÃ¨mes** : RÃ©partition par catÃ©gorie (FIBRE, MOBILE, etc.)
6. **Types d'Incidents** : Distribution des types d'incidents dÃ©tectÃ©s
7. **Temps de Traitement** : Performance du systÃ¨me
8. **Taux de SuccÃ¨s** : Pourcentage de tweets classifiÃ©s avec succÃ¨s
9. **Volume TraitÃ©** : Nombre total de tweets analysÃ©s
10. **Tendances Temporelles** : Ã‰volution dans le temps (si donnÃ©es temporelles disponibles)

---

## ğŸ”§ Documentation Technique

### Structure du Projet

```
FreeMobilaChat/
â”‚
â”œâ”€â”€ streamlit_app/              # Application principale
â”‚   â”œâ”€â”€ app.py                  # Point d'entrÃ©e principal
â”‚   â”œâ”€â”€ config.py               # Configuration centralisÃ©e
â”‚   â”‚
â”‚   â”œâ”€â”€ pages/                  # Pages de l'application
â”‚   â”‚   â””â”€â”€ Classification_Mistral.py  # Page de classification
â”‚   â”‚
â”‚   â”œâ”€â”€ components/             # Composants UI rÃ©utilisables
â”‚   â”‚   â”œâ”€â”€ auth_forms.py       # Formulaires d'authentification
â”‚   â”‚   â””â”€â”€ charts.py           # Composants de visualisation
â”‚   â”‚
â”‚   â”œâ”€â”€ services/               # Logique mÃ©tier
â”‚   â”‚   â”œâ”€â”€ auth_service.py     # Service d'authentification
â”‚   â”‚   â”œâ”€â”€ mistral_classifier.py      # Classificateur Mistral
â”‚   â”‚   â”œâ”€â”€ bert_classifier.py         # Classificateur BERT
â”‚   â”‚   â”œâ”€â”€ rule_classifier.py        # Classificateur par rÃ¨gles
â”‚   â”‚   â”œâ”€â”€ multi_model_orchestrator.py  # Orchestrateur multi-modÃ¨les
â”‚   â”‚   â”œâ”€â”€ tweet_cleaner.py           # Nettoyage de tweets
â”‚   â”‚   â”œâ”€â”€ gemini_classifier.py       # Classificateur Gemini
â”‚   â”‚   â”œâ”€â”€ enhanced_kpis_vizualizations.py  # KPIs avancÃ©s
â”‚   â”‚   â”œâ”€â”€ ultra_optimized_classifier.py   # Classificateur optimisÃ©
â”‚   â”‚   â””â”€â”€ role_manager.py            # Gestion des rÃ´les
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Fonctions utilitaires
â”‚       â”œâ”€â”€ helpers.py
â”‚       â””â”€â”€ validators.py
â”‚
â”œâ”€â”€ scripts/                    # Scripts utilitaires
â”‚   â”œâ”€â”€ run_tests.sh           # Script de tests
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/                       # Datasets
â”‚   â”œâ”€â”€ training/              # DonnÃ©es d'entraÃ®nement
â”‚   â”œâ”€â”€ validation/            # DonnÃ©es de validation
â”‚   â””â”€â”€ test/                  # DonnÃ©es de test
â”‚
â”œâ”€â”€ models/                     # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ bert_finetuned/        # BERT fine-tunÃ©
â”‚   â””â”€â”€ embeddings/           # Embeddings cachÃ©s
â”‚
â”œâ”€â”€ tests/                      # Tests
â”‚   â”œâ”€â”€ unit/                  # Tests unitaires
â”‚   â”œâ”€â”€ integration/           # Tests d'intÃ©gration
â”‚   â””â”€â”€ e2e/                   # Tests end-to-end
â”‚
â”œâ”€â”€ requirements-streamlit.txt  # DÃ©pendances production
â”œâ”€â”€ requirements.txt            # DÃ©pendances complÃ¨tes
â”œâ”€â”€ README.md                   # Ce fichier
â””â”€â”€ LICENSE                     # Licence MIT
```

### Stack Technologique

#### Framework Core
- **Python** : 3.11+
- **Streamlit** : 1.51.0 (Framework web interactif)

#### Machine Learning
- **Transformers** : 4.44.2 (Hugging Face) - ModÃ¨les BERT
- **PyTorch** : 2.4.1 - Backend pour Transformers
- **Sentence-Transformers** : 3.1.1 - Embeddings sÃ©mantiques
- **Scikit-learn** : 1.5.2 - Outils ML complÃ©mentaires

#### NLP & Language Models
- **Mistral AI** : Via Ollama (modÃ¨les locaux)
- **BERT-base-multilingual** : ModÃ¨le prÃ©-entraÃ®nÃ© Hugging Face
- **Gemini API** : Google Generative AI (alternative cloud)
- **spaCy** : 3.8.2 - Traitement NLP supplÃ©mentaire

#### Traitement de DonnÃ©es
- **Pandas** : 2.2.3 - Manipulation de donnÃ©es
- **NumPy** : 2.1.1 - Calculs numÃ©riques

#### Visualisation
- **Plotly** : 5.24.1 - Graphiques interactifs

#### Authentification & SÃ©curitÃ©
- **bcrypt** : 4.0.1 - Hachage de mots de passe
- **PyJWT** : 2.8.0 - Tokens JWT

---

## ğŸ§ª Tests

### ExÃ©cution des Tests

```bash
# Tous les tests
./scripts/run_tests.sh all

# Tests unitaires uniquement
./scripts/run_tests.sh unit

# Tests d'intÃ©gration (nÃ©cessite GEMINI_API_KEY)
./scripts/run_tests.sh integration

# Tests avec couverture
pytest tests/ --cov=streamlit_app --cov-report=html
```

### Ã‰valuation des Performances

```bash
# Ã‰valuer sur dataset de test
python scripts/evaluate_model.py --dataset tests/data/test_dataset.csv

# GÃ©nÃ©ration de rapport HTML
python scripts/generate_report.py
```

---

## âš™ï¸ Configuration

### Variables d'Environnement

CrÃ©ez un fichier `.env` Ã  la racine du projet :

```env
# Configuration Mistral/Ollama
OLLAMA_BASE_URL=http://localhost:11434
MISTRAL_MODEL=mistral:latest

# Configuration Gemini API (Optionnel)
GEMINI_API_KEY=your_gemini_api_key_here

# Configuration Application
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=INFO
BACKEND_URL=http://localhost:8000
```

### Configuration Streamlit

Le fichier `.streamlit/config.toml` contient :

```toml
[server]
port = 8503
enableCORS = false
enableXsrfProtection = true
maxUploadSize = 200

[theme]
primaryColor = "#667eea"
backgroundColor = "#ffffff"
secondaryBackgroundColor = "#f0f2f6"
```

---

## ğŸ¤ Contributions

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  soumettre une Pull Request.

1. Forkez le repository
2. CrÃ©ez votre branche de fonctionnalitÃ© (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Poussez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

---

## ğŸ“„ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

### Usage AcadÃ©mique

Ce projet est soumis dans le cadre d'un mÃ©moire de Master. Si vous utilisez ce travail dans une recherche acadÃ©mique, veuillez citer :

```bibtex
@mastersthesis{archimed2025freemobilachat,
  title={Multi-Model Sentiment Analysis for Telecommunications Customer Service},
  author={Archimed, Anderson},
  year={2025},
  school={[Votre UniversitÃ©]},
  type={Master's Thesis}
}
```

---

## ğŸ“ Contact & Support

**Auteur** : Anderson Archimed  
**GitHub** : [@Archimedh-Anderson](https://github.com/Archimedh-Anderson)  
**Repository** : https://github.com/Archimedh-Anderson/FreeMobileApp  
**DÃ©mo Live** : https://freemobilachat.streamlit.app

Pour les questions acadÃ©miques ou opportunitÃ©s de collaboration, contactez via GitHub.

---

## ğŸ™ Remerciements

Remerciements spÃ©ciaux Ã  :
- **Hugging Face** pour les modÃ¨les transformer et l'infrastructure
- **Streamlit** pour l'excellent framework d'application
- **Mistral AI** pour les puissants modÃ¨les de langage
- **CommunautÃ© Open Source** pour les outils et bibliothÃ¨ques inestimables

---

<div align="center">

**DerniÃ¨re Mise Ã  Jour** : Janvier 2025  
**Version** : 2.0.0  
**Statut** : Production Ready - Soumission AcadÃ©mique

Fait avec â¤ï¸ pour la communautÃ© NLP/ML

</div>
