"""
Module de Nettoyage de Tweets - FreeMobilaChat
===============================================

Module avanc√© pour le pr√©traitement et la d√©duplication de tweets.
Conforme aux sp√©cifications techniques du projet acad√©mique.

Fonctionnalit√©s:
- Suppression des doublons (hash MD5)
- Nettoyage du texte (URLs, mentions, hashtags, emojis)
- Normalisation unicode
- Statistiques de nettoyage
"""

# Imports pour la manipulation de types et de donn√©es
from typing import Tuple, Dict, List, Optional  # Annotations de types pour la clart√© du code
import pandas as pd  # Traitement de donn√©es tabulaires (DataFrames)
import hashlib  # G√©n√©ration de hash MD5 pour d√©tection de doublons
import re  # Expressions r√©guli√®res pour le nettoyage de texte
from unidecode import unidecode  # Normalisation des caract√®res unicode vers ASCII
import emoji  # Conversion des emojis en repr√©sentation textuelle
import logging  # Journalisation des op√©rations de nettoyage

# Configuration du logger pour le suivi des op√©rations
logger = logging.getLogger(__name__)

# D√©finition des patterns regex pour le nettoyage (conformes aux sp√©cifications)
URL_PATTERN = r'http\S+|www\S+|https\S+'  # D√©tection de toutes les URLs (http, https, www)
MENTION_PATTERN = r'@\w+'  # D√©tection des mentions Twitter (@username)
HASHTAG_PATTERN = r'#\w+'  # D√©tection des hashtags (#tag)
PUNCTUATION_PATTERN = r'[^\w\s,.\?!]'  # Suppression ponctuation except√©e (garde , . ? !)
WHITESPACE_PATTERN = r'\s+'  # Normalisation des espaces multiples en espace unique


DEFAULT_DOMAIN_KEYWORDS = [
    "free", "freebox", "free mobile", "freebox delta", "freebox pop",
    "fibre", "fiber", "connexion", "connection", "reseau", "r√©seau",
    "4g", "5g", "data", "debit", "d√©bit", "facture", "facturation",
    "reclamation", "r√©clamation", "incident", "panne", "bug", "sav",
    "support", "service client", "assistance", "wifi", "box", "modem"
]


class TweetCleaner:
    """
    Nettoyage et d√©duplication de tweets
    
    Cette classe impl√©mente un pipeline complet de nettoyage selon les
    sp√©cifications techniques du projet. Elle garantit des donn√©es
    propres et d√©duplic√©es pour la classification.
    """
    
    def __init__(self, 
                 remove_urls: bool = True,
                 remove_mentions: bool = True,
                 remove_hashtags: bool = False,
                 convert_emojis: bool = True,
                 normalize_unicode: bool = True,
                 lowercase: bool = True,
                 preserve_domain_keywords: bool = True,
                 extra_stopwords: Optional[List[str]] = None):
        """
        Initialise le nettoyeur de tweets
        
        Args:
            remove_urls: Supprimer les URLs
            remove_mentions: Supprimer les mentions @username
            remove_hashtags: Supprimer les hashtags #tag
            convert_emojis: Convertir les emojis en texte
            normalize_unicode: Normaliser les caract√®res unicode
            lowercase: Forcer en minuscules pour homog√©n√©it√©
            preserve_domain_keywords: Conserver les mots-cl√©s Free Mobile
            extra_stopwords: Liste personnalis√©e de stopwords √† supprimer
        """
        self.remove_urls = remove_urls
        self.remove_mentions = remove_mentions
        self.remove_hashtags = remove_hashtags
        self.convert_emojis = convert_emojis
        self.normalize_unicode = normalize_unicode
        self.lowercase = lowercase
        self.preserve_domain_keywords = preserve_domain_keywords
        self.extra_stopwords = set(s.lower() for s in (extra_stopwords or []))
        self.domain_keywords = set(DEFAULT_DOMAIN_KEYWORDS)
        
        logger.info(f"TweetCleaner initialis√© avec options: URLs={remove_urls}, Mentions={remove_mentions}, Hashtags={remove_hashtags}")
    
    @staticmethod
    def remove_duplicates(df: pd.DataFrame, text_column: str = 'text') -> pd.DataFrame:
        """
        Suppression des doublons par hash MD5
        
        Utilise un hash MD5 du texte pour identifier les tweets identiques,
        m√™me avec des variations mineures.
        
        Args:
            df: DataFrame avec tweets
            text_column: Nom de la colonne texte
            
        Returns:
            DataFrame sans doublons
        """
        if text_column not in df.columns:
            logger.warning(f"Colonne '{text_column}' non trouv√©e, pas de d√©duplication")
            return df
        
        # Cr√©ation des hash MD5
        df['_hash'] = df[text_column].apply(
            lambda x: hashlib.md5(str(x).encode()).hexdigest() if pd.notna(x) else None
        )
        
        # Comptage avant
        count_before = len(df)
        
        # Suppression des doublons bas√©e sur le hash
        df_dedup = df.drop_duplicates(subset=['_hash'], keep='first')
        
        # Suppression de la colonne temporaire
        df_dedup = df_dedup.drop(columns=['_hash'])
        
        # Comptage apr√®s
        count_after = len(df_dedup)
        duplicates_removed = count_before - count_after
        
        logger.info(f"D√©duplication: {duplicates_removed} doublons supprim√©s ({count_before} ‚Üí {count_after})")
        
        return df_dedup.reset_index(drop=True)
    
    def clean_text(self, text: str) -> str:
        """
        Nettoyage complet d'un tweet
        
        Ordre des op√©rations (conforme aux specs):
        1. Suppression URLs (http/https/www)
        2. Suppression mentions (@username)
        3. Suppression hashtags (#tag)
        4. Conversion emojis en texte
        5. Normalisation unicode
        6. Suppression ponctuation excessive
        7. Normalisation espaces
        
        Args:
            text: Tweet brut
            
        Returns:
            Tweet nettoy√©
        """
        if not isinstance(text, str) or pd.isna(text):
            return ""
        
        cleaned = text
        
        if self.lowercase:
            cleaned = cleaned.lower()
        
        # 1. Suppression des URLs
        if self.remove_urls:
            cleaned = re.sub(URL_PATTERN, '', cleaned)
        
        # 2. Suppression des mentions
        if self.remove_mentions:
            cleaned = re.sub(MENTION_PATTERN, '', cleaned)
        
        # 3. Suppression des hashtags
        if self.remove_hashtags:
            cleaned = re.sub(HASHTAG_PATTERN, '', cleaned)
        
        # 4. Conversion des emojis en texte
        if self.convert_emojis:
            try:
                cleaned = emoji.demojize(cleaned, delimiters=(" ", " "))
            except:
                pass  # Si emoji pose probl√®me, continuer
        
        # 5. Normalisation unicode
        if self.normalize_unicode:
            try:
                cleaned = unidecode(cleaned)
            except:
                pass  # Si unidecode pose probl√®me, continuer
        
        # 6. Suppression ponctuation excessive (garder , . ? !)
        # cleaned = re.sub(PUNCTUATION_PATTERN, '', cleaned)
        
        # 7. Normalisation des espaces
        cleaned = re.sub(WHITESPACE_PATTERN, ' ', cleaned)
        cleaned = cleaned.strip()
        
        # 8. Suppression des stopwords suppl√©mentaires
        if self.extra_stopwords:
            tokens = []
            for token in cleaned.split():
                if token not in self.extra_stopwords:
                    tokens.append(token)
            cleaned = " ".join(tokens)
        
        # 9. Optionnel: pr√©server mots-cl√©s m√©tiers (en les r√©-injectant si supprim√©s)
        if self.preserve_domain_keywords and cleaned:
            preserved_tokens = []
            for keyword in self.domain_keywords:
                if keyword in text.lower() and keyword not in cleaned:
                    preserved_tokens.append(keyword.replace(" ", "_"))
            if preserved_tokens:
                cleaned = f"{cleaned} {' '.join(preserved_tokens)}".strip()
        
        return cleaned
    
    def process_dataframe(self, df: pd.DataFrame, text_column: str = 'text') -> Tuple[pd.DataFrame, Dict]:
        """
        Pipeline complet de nettoyage
        
        Applique toutes les op√©rations de nettoyage et g√©n√®re des statistiques
        d√©taill√©es sur le processus.
        
        Args:
            df: DataFrame brut
            text_column: Colonne √† nettoyer
            
        Returns:
            (df_cleaned, stats_dict) - DataFrame nettoy√© et statistiques
        """
        logger.info(f"D√©marrage du nettoyage de {len(df)} tweets")
        
        # Statistiques initiales
        stats = {
            'total_original': len(df),
            'empty_tweets': 0,
            'duplicates_removed': 0,
            'total_cleaned': 0,
            'avg_length_before': 0,
            'avg_length_after': 0,
            'cleaning_operations': []
        }
        
        # V√©rifier que la colonne existe
        if text_column not in df.columns:
            logger.error(f"Colonne '{text_column}' non trouv√©e dans le DataFrame")
            return df, stats
        
        # Copie pour ne pas modifier l'original
        df_clean = df.copy()
        
        # 1. Suppression des valeurs manquantes
        empty_count = df_clean[text_column].isna().sum()
        df_clean = df_clean.dropna(subset=[text_column])
        stats['empty_tweets'] = int(empty_count)
        stats['cleaning_operations'].append(f"Valeurs manquantes supprim√©es: {empty_count}")
        
        # 2. Suppression des doublons
        count_before_dedup = len(df_clean)
        df_clean = self.remove_duplicates(df_clean, text_column)
        duplicates = count_before_dedup - len(df_clean)
        stats['duplicates_removed'] = int(duplicates)
        stats['cleaning_operations'].append(f"Doublons supprim√©s: {duplicates}")
        
        # 3. Calcul de la longueur moyenne avant nettoyage
        if len(df_clean) > 0:
            stats['avg_length_before'] = float(df_clean[text_column].astype(str).str.len().mean())
        else:
            stats['avg_length_before'] = 0.0
        
        # 4. Nettoyage du texte
        df_clean[f'{text_column}_cleaned'] = df_clean[text_column].apply(self.clean_text)
        
        # 5. Calcul de la longueur moyenne apr√®s nettoyage
        if len(df_clean) > 0:
            stats['avg_length_after'] = float(df_clean[f'{text_column}_cleaned'].str.len().mean())
        else:
            stats['avg_length_after'] = 0.0
        
        # 6. Suppression des tweets vides apr√®s nettoyage
        if len(df_clean) > 0:
            df_clean = df_clean[df_clean[f'{text_column}_cleaned'].str.len() > 0]
        
        # Statistiques finales
        stats['total_cleaned'] = len(df_clean)
        stats['cleaning_operations'].append(f"Tweets nettoy√©s: {len(df_clean)}")
        
        logger.info(f"Nettoyage termin√©: {stats['total_original']} ‚Üí {stats['total_cleaned']} tweets")
        
        return df_clean.reset_index(drop=True), stats
    
    def get_cleaning_report(self, stats: Dict) -> str:
        """
        G√©n√®re un rapport de nettoyage format√©
        
        Args:
            stats: Dictionnaire de statistiques
            
        Returns:
            Rapport format√© en markdown
        """
        report = f"""
## üßπ Rapport de Nettoyage

**Tweets originaux:** {stats['total_original']:,}
**Tweets nettoy√©s:** {stats['total_cleaned']:,}
**Tweets supprim√©s:** {stats['total_original'] - stats['total_cleaned']:,}

### D√©tails
- **Valeurs manquantes:** {stats['empty_tweets']}
- **Doublons:** {stats['duplicates_removed']}
- **Longueur moyenne avant:** {stats['avg_length_before']:.1f} caract√®res
- **Longueur moyenne apr√®s:** {stats['avg_length_after']:.1f} caract√®res

### Op√©rations effectu√©es
"""
        for op in stats['cleaning_operations']:
            report += f"- {op}\n"
        
        return report


# Fonctions utilitaires
def clean_tweet_text(text: str, 
                     remove_urls: bool = True,
                     remove_mentions: bool = True,
                     remove_hashtags: bool = False,
                     lowercase: bool = True) -> str:
    """
    Fonction helper pour nettoyer un tweet unique
    
    Args:
        text: Texte du tweet
        remove_urls: Supprimer les URLs
        remove_mentions: Supprimer les mentions
        remove_hashtags: Supprimer les hashtags
        
    Returns:
        Texte nettoy√©
    """
    cleaner = TweetCleaner(
        remove_urls=remove_urls,
        remove_mentions=remove_mentions,
        remove_hashtags=remove_hashtags,
        lowercase=lowercase
    )
    return cleaner.clean_text(text)


def batch_clean_tweets(tweets: List[str], **kwargs) -> List[str]:
    """
    Nettoie un lot de tweets
    
    Args:
        tweets: Liste de tweets √† nettoyer
        **kwargs: Options de nettoyage
        
    Returns:
        Liste de tweets nettoy√©s
    """
    cleaner = TweetCleaner(**kwargs)
    return [cleaner.clean_text(tweet) for tweet in tweets]

