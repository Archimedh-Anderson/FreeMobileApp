"""
Script de GÃ©nÃ©ration du Dataset d'EntraÃ®nement Complet
=======================================================

GÃ©nÃ¨re un nouveau dataset d'entraÃ®nement avec TOUS les KPIs Ã  partir de free_tweet_export.csv

Colonnes gÃ©nÃ©rÃ©es:
- sentiment (positif/neutre/nÃ©gatif)
- catÃ©gorie (thÃ¨me principal)
- priority (basse/moyenne/haute/critique)
- urgent (True/False)
- besoin_reponse (True/False)
- estimation_resolution (en heures)
- rÃ©clamations (oui/non)

DÃ©veloppÃ© pour mÃ©moire de Master en Data Science
Date: 2025-11-08
"""

import sys
import os
sys.path.insert(0, 'streamlit_app')

import pandas as pd
import numpy as np
from datetime import datetime
from tqdm import tqdm
import logging

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import des services
from services.tweet_cleaner import TweetCleaner
from services.ultra_optimized_classifier import UltraOptimizedClassifier

class TrainingDatasetGenerator:
    """GÃ©nÃ©rateur de dataset d'entraÃ®nement avec tous les KPIs"""
    
    def __init__(self, input_file: str, output_file: str):
        self.input_file = input_file
        self.output_file = output_file
        self.cleaner = TweetCleaner()
        self.classifier = None
        
    def load_data(self) -> pd.DataFrame:
        """Charge le dataset brut"""
        logger.info(f"ğŸ“‚ Chargement de {self.input_file}...")
        df = pd.read_csv(self.input_file)
        logger.info(f"âœ… {len(df):,} tweets chargÃ©s")
        return df
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Nettoie les donnÃ©es"""
        logger.info("ğŸ§¹ Nettoyage des donnÃ©es...")
        
        # Utiliser TweetCleaner
        df_cleaned, stats = self.cleaner.process_dataframe(df.copy(), 'text')
        
        logger.info(f"âœ… Nettoyage terminÃ©:")
        logger.info(f"   - Original: {stats.get('total_original', 0):,} tweets")
        logger.info(f"   - NettoyÃ©: {stats.get('total_cleaned', 0):,} tweets")
        logger.info(f"   - Doublons retirÃ©s: {stats.get('duplicates_removed', 0):,}")
        
        return df_cleaned
    
    def classify_data(self, df: pd.DataFrame, mode: str = 'balanced') -> pd.DataFrame:
        """Classifie les donnÃ©es avec tous les KPIs"""
        logger.info(f"ğŸ¤– Classification en mode {mode.upper()}...")
        logger.info(f"   Dataset: {len(df):,} tweets")
        
        # Initialiser le classificateur ultra-optimisÃ©
        self.classifier = UltraOptimizedClassifier(
            batch_size=50,
            max_workers=4,
            use_cache=True,
            enable_logging=True
        )
        
        # Progress callback
        def progress_callback(message, progress):
            logger.info(f"   {message} ({progress*100:.0f}%)")
        
        # Classification
        df_classified, benchmark = self.classifier.classify_tweets_batch(
            df,
            'text_cleaned',
            mode=mode,
            progress_callback=progress_callback
        )
        
        logger.info(f"âœ… Classification terminÃ©e:")
        logger.info(f"   - Temps: {benchmark.total_time_seconds:.1f}s")
        logger.info(f"   - Vitesse: {benchmark.tweets_per_second:.1f} tweets/s")
        
        return df_classified
    
    def add_training_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ajoute et formate les colonnes d'entraÃ®nement"""
        logger.info("ğŸ“‹ GÃ©nÃ©ration des colonnes d'entraÃ®nement...")
        
        df_training = df.copy()
        
        # 1. Sentiment (dÃ©jÃ  prÃ©sent)
        if 'sentiment' not in df_training.columns:
            df_training['sentiment'] = 'neutre'
        
        # 2. CatÃ©gorie (renommer 'topics' si prÃ©sent)
        if 'topics' in df_training.columns:
            df_training['catÃ©gorie'] = df_training['topics']
        else:
            df_training['catÃ©gorie'] = 'autre'
        
        # 3. Priority (basÃ©e sur urgence)
        if 'urgence' in df_training.columns:
            df_training['priority'] = df_training['urgence'].map({
                'faible': 'basse',
                'moyenne': 'moyenne',
                'critique': 'haute'
            }).fillna('basse')
        else:
            df_training['priority'] = 'basse'
        
        # 4. Urgent (True/False basÃ© sur urgence)
        if 'urgence' in df_training.columns:
            df_training['urgent'] = df_training['urgence'].isin(['critique'])
        else:
            df_training['urgent'] = False
        
        # 5. Besoin_reponse (basÃ© sur sentiment et is_claim)
        if 'sentiment' in df_training.columns and 'is_claim' in df_training.columns:
            df_training['besoin_reponse'] = (
                (df_training['sentiment'] == 'negatif') | 
                (df_training['is_claim'] == 'oui')
            )
        else:
            df_training['besoin_reponse'] = True
        
        # 6. Estimation_resolution (en heures, basÃ©e sur priority et urgence)
        def calculate_resolution_time(row):
            if row.get('urgent', False):
                return 2  # 2 heures pour urgent
            elif row.get('priority', 'basse') == 'haute':
                return 24  # 24h pour haute prioritÃ©
            elif row.get('priority', 'basse') == 'moyenne':
                return 48  # 48h pour moyenne
            else:
                return 72  # 72h pour basse
        
        df_training['estimation_resolution'] = df_training.apply(calculate_resolution_time, axis=1)
        
        # 7. RÃ©clamations (basÃ© sur is_claim)
        if 'is_claim' in df_training.columns:
            df_training['rÃ©clamations'] = df_training['is_claim']
        else:
            df_training['rÃ©clamations'] = 'non'
        
        logger.info(f"âœ… Colonnes gÃ©nÃ©rÃ©es:")
        logger.info(f"   - sentiment: {df_training['sentiment'].nunique()} valeurs uniques")
        logger.info(f"   - catÃ©gorie: {df_training['catÃ©gorie'].nunique()} valeurs uniques")
        logger.info(f"   - priority: {df_training['priority'].nunique()} valeurs uniques")
        logger.info(f"   - urgent: {df_training['urgent'].sum()} tweets urgents")
        logger.info(f"   - besoin_reponse: {df_training['besoin_reponse'].sum()} tweets nÃ©cessitant rÃ©ponse")
        logger.info(f"   - estimation_resolution: Moyenne {df_training['estimation_resolution'].mean():.1f}h")
        logger.info(f"   - rÃ©clamations: {(df_training['rÃ©clamations'] == 'oui').sum()} rÃ©clamations")
        
        return df_training
    
    def select_training_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """SÃ©lectionne les colonnes finales pour l'entraÃ®nement"""
        
        # Colonnes de base Ã  conserver
        base_columns = ['tweet_id', 'author', 'text', 'date', 'url']
        
        # Colonnes d'entraÃ®nement demandÃ©es
        training_columns = [
            'sentiment',
            'catÃ©gorie',
            'priority',
            'urgent',
            'besoin_reponse',
            'estimation_resolution',
            'rÃ©clamations'
        ]
        
        # Colonnes supplÃ©mentaires utiles
        extra_columns = ['confidence', 'text_cleaned']
        
        # SÃ©lectionner les colonnes disponibles
        available_base = [col for col in base_columns if col in df.columns]
        available_training = [col for col in training_columns if col in df.columns]
        available_extra = [col for col in extra_columns if col in df.columns]
        
        selected_columns = available_base + available_training + available_extra
        
        df_final = df[selected_columns].copy()
        
        logger.info(f"ğŸ“‹ Colonnes finales sÃ©lectionnÃ©es: {len(selected_columns)}")
        logger.info(f"   {', '.join(selected_columns)}")
        
        return df_final
    
    def generate(self, mode: str = 'balanced', min_tweets: int = 2600, max_tweets: int = 3500):
        """GÃ©nÃ¨re le dataset d'entraÃ®nement complet"""
        
        print("\n" + "="*80)
        print("  GÃ‰NÃ‰RATION DATASET D'ENTRAÃNEMENT COMPLET")
        print("  Classification Mistral - Tous les KPIs")
        print("="*80 + "\n")
        
        # 1. Chargement
        df = self.load_data()
        
        # 2. Nettoyage
        df_cleaned = self.clean_data(df)
        
        # 3. VÃ©rifier la taille
        if len(df_cleaned) < min_tweets:
            logger.warning(f"âš ï¸  Dataset trop petit aprÃ¨s nettoyage: {len(df_cleaned)} < {min_tweets}")
            logger.warning(f"   On continue quand mÃªme...")
        
        # 4. Limiter si nÃ©cessaire
        if len(df_cleaned) > max_tweets:
            logger.info(f"ğŸ“Š Ã‰chantillonnage de {len(df_cleaned):,} Ã  {max_tweets:,} tweets...")
            df_cleaned = df_cleaned.sample(n=max_tweets, random_state=42).reset_index(drop=True)
        
        # 5. Classification avec tous les KPIs
        df_classified = self.classify_data(df_cleaned, mode=mode)
        
        # 6. Ajout des colonnes d'entraÃ®nement
        df_training = self.add_training_columns(df_classified)
        
        # 7. SÃ©lection des colonnes finales
        df_final = self.select_training_columns(df_training)
        
        # 8. Sauvegarde
        logger.info(f"ğŸ’¾ Sauvegarde dans {self.output_file}...")
        df_final.to_csv(self.output_file, index=False, encoding='utf-8')
        
        # 9. Statistiques finales
        print("\n" + "="*80)
        print("  âœ… DATASET D'ENTRAÃNEMENT GÃ‰NÃ‰RÃ‰ AVEC SUCCÃˆS")
        print("="*80 + "\n")
        
        print(f"ğŸ“Š STATISTIQUES:")
        print(f"   - Tweets originaux:     {len(df):,}")
        print(f"   - Tweets nettoyÃ©s:      {len(df_cleaned):,}")
        print(f"   - Tweets finaux:        {len(df_final):,}")
        print(f"   - Colonnes:             {len(df_final.columns)}")
        
        print(f"\nğŸ“‹ COLONNES GÃ‰NÃ‰RÃ‰ES:")
        for i, col in enumerate(df_final.columns, 1):
            print(f"   {i:2d}. {col}")
        
        print(f"\nğŸ¯ KPIs:")
        if 'sentiment' in df_final.columns:
            print(f"   - Sentiment positif:    {(df_final['sentiment'] == 'positif').sum():,} ({(df_final['sentiment'] == 'positif').sum()/len(df_final)*100:.1f}%)")
            print(f"   - Sentiment neutre:     {(df_final['sentiment'] == 'neutre').sum():,} ({(df_final['sentiment'] == 'neutre').sum()/len(df_final)*100:.1f}%)")
            print(f"   - Sentiment nÃ©gatif:    {(df_final['sentiment'] == 'negatif').sum():,} ({(df_final['sentiment'] == 'negatif').sum()/len(df_final)*100:.1f}%)")
        
        if 'rÃ©clamations' in df_final.columns:
            reclamations = (df_final['rÃ©clamations'] == 'oui').sum()
            print(f"   - RÃ©clamations:         {reclamations:,} ({reclamations/len(df_final)*100:.1f}%)")
        
        if 'urgent' in df_final.columns:
            urgent = df_final['urgent'].sum()
            print(f"   - Tweets urgents:       {urgent:,} ({urgent/len(df_final)*100:.1f}%)")
        
        if 'besoin_reponse' in df_final.columns:
            besoin = df_final['besoin_reponse'].sum()
            print(f"   - Besoin rÃ©ponse:       {besoin:,} ({besoin/len(df_final)*100:.1f}%)")
        
        if 'estimation_resolution' in df_final.columns:
            print(f"   - RÃ©solution moyenne:   {df_final['estimation_resolution'].mean():.1f}h")
        
        print(f"\nğŸ’¾ FICHIER GÃ‰NÃ‰RÃ‰:")
        print(f"   {self.output_file}")
        
        file_size = os.path.getsize(self.output_file) / 1024 / 1024
        print(f"   Taille: {file_size:.2f} MB")
        
        print("\n" + "="*80)
        print("  ğŸ‰ GÃ‰NÃ‰RATION TERMINÃ‰E AVEC SUCCÃˆS")
        print("="*80 + "\n")
        
        return df_final


def main():
    """Fonction principale"""
    
    # Configuration
    input_file = "data/raw/free_tweet_export.csv"
    output_file = "data/training/train_dataset.csv"
    
    # Mode de classification
    mode = 'balanced'  # balanced = meilleur compromis vitesse/prÃ©cision
    
    # Objectif de taille
    min_tweets = 2600
    max_tweets = 3500
    
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                                                                                        â•‘")
    print("â•‘           ğŸ“Š GÃ‰NÃ‰RATION DATASET D'ENTRAÃNEMENT AVEC TOUS LES KPIs                     â•‘")
    print("â•‘                                                                                        â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    print(f"ğŸ“ CONFIGURATION:")
    print(f"   - Fichier source:    {input_file}")
    print(f"   - Fichier sortie:    {output_file}")
    print(f"   - Mode:              {mode.upper()} (88% prÃ©cision, ~2 min)")
    print(f"   - Objectif taille:   {min_tweets:,} - {max_tweets:,} tweets")
    
    print(f"\nğŸ¯ COLONNES Ã€ GÃ‰NÃ‰RER:")
    colonnes = [
        "sentiment",
        "catÃ©gorie",
        "priority",
        "urgent",
        "besoin_reponse",
        "estimation_resolution",
        "rÃ©clamations"
    ]
    for i, col in enumerate(colonnes, 1):
        print(f"   {i}. {col}")
    
    print("\n" + "-"*80 + "\n")
    
    try:
        # VÃ©rifier l'existence du fichier source
        if not os.path.exists(input_file):
            logger.error(f"âŒ Fichier source non trouvÃ©: {input_file}")
            return
        
        # GÃ©nÃ©rer le dataset
        generator = TrainingDatasetGenerator(input_file, output_file)
        df_final = generator.generate(mode=mode, min_tweets=min_tweets, max_tweets=max_tweets)
        
        # Validation finale
        print("\nâœ… VALIDATION FINALE:")
        print(f"   - Dataset gÃ©nÃ©rÃ©: {len(df_final):,} tweets")
        
        if min_tweets <= len(df_final) <= max_tweets:
            print(f"   - Taille cible: âœ… OK ({min_tweets:,} - {max_tweets:,})")
        else:
            print(f"   - Taille cible: âš ï¸  Hors objectif")
        
        # VÃ©rifier toutes les colonnes
        colonnes_requises = colonnes
        colonnes_presentes = [col for col in colonnes_requises if col in df_final.columns]
        
        print(f"   - Colonnes requises: {len(colonnes_presentes)}/{len(colonnes_requises)}")
        
        if len(colonnes_presentes) == len(colonnes_requises):
            print(f"   - Toutes les colonnes: âœ… PrÃ©sentes")
        else:
            manquantes = [col for col in colonnes_requises if col not in df_final.columns]
            print(f"   - Colonnes manquantes: {', '.join(manquantes)}")
        
        print("\nğŸ“– PROCHAINES Ã‰TAPES:")
        print(f"   1. VÃ©rifier le fichier: {output_file}")
        print(f"   2. Utiliser ce dataset pour l'entraÃ®nement du modÃ¨le")
        print(f"   3. GÃ©nÃ©rer les datasets de validation et test si nÃ©cessaire")
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la gÃ©nÃ©ration: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)








